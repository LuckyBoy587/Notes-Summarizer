{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f2c9083",
   "metadata": {},
   "source": [
    "# Notes Summarizer (Colab)\n",
    "\n",
    "This notebook provides a minimal Gradio UI to run the Notes Summarizer in Google Colab.\n",
    "\n",
    "Run the cells in order. Install the packages in Colab (see the requirements cell). The notebook will import `get_model_tokenizer_device` from the project's `config.py` and will use GPU if available.\n",
    "\n",
    "Costs: model loading and inference on Colab GPU consumes compute and may be subject to Colab quotas. If you run out-of-memory, reduce max_length/num_return_sequences or use CPU (uncheck use_fp16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4918f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install gradio\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea58dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone or update the GitHub repo into Colab and add it to sys.path\n",
    "# Replace repo_url with your repo URL if different. If the repo is private, mount Drive or use an authenticated method.\n",
    "repo_url = \"https://github.com/LuckyBoy587/Notes-Summarizer.git\"\n",
    "repo_dir = \"/content/Notes-Summarizer\"\n",
    "\n",
    "import os, sys, subprocess\n",
    "\n",
    "print('Repository URL:', repo_url)\n",
    "if os.path.exists(repo_dir):\n",
    "    print(f\"{repo_dir} already exists. Attempting 'git pull' to update.\")\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"-C\", repo_dir, \"pull\"], check=True)\n",
    "        print('Pulled latest changes.')\n",
    "    except Exception as e:\n",
    "        print('git pull failed:', e)\n",
    "else:\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"clone\", repo_url, repo_dir], check=True)\n",
    "        print('Cloned into', repo_dir)\n",
    "    except Exception as e:\n",
    "        print('git clone failed:', e)\n",
    "\n",
    "# Add the repo directory to sys.path so config.py is importable\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.insert(0, repo_dir)\n",
    "print('Added to sys.path:', sys.path[0])\n",
    "\n",
    "print('\\nNotes:')\n",
    "print('- If the repository is private, you will need to authenticate (use an access token or mount Google Drive with the repo).')\n",
    "print('- After this cell runs, you should be able to import from config.py (e.g., from config import get_model_tokenizer_device)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and a reminder to set working directory in Colab so config.py is importable\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "import time\n",
    "print('Ensure the project files (including config.py) are available in the Colab working directory or add the path to sys.path.')\n",
    "# Example (uncomment in Colab if you mount drive or upload repo):\n",
    "# sys.path.insert(0, '/content/your-repo-path')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions: file loading, cleaning, zip creation\n",
    "import io, zipfile, re\n",
    "from typing import List\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = re.sub(r'\\r\\n', '\\n', text)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    text = re.sub(r'[ \\t]{2,}', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_text_from_uploaded(file) -> str:\n",
    "    # file may be a temporary file with .name attribute or a file-like object\n",
    "    path = getattr(file, 'name', None)\n",
    "    if path and os.path.exists(path):\n",
    "        ext = Path(path).suffix.lower()\n",
    "        if ext in ('.txt', '.md'):\n",
    "            return Path(path).read_text(encoding='utf-8', errors='ignore')\n",
    "        if ext == '.pdf':\n",
    "            try:\n",
    "                import PyPDF2\n",
    "            except Exception:\n",
    "                raise RuntimeError('PyPDF2 is required to extract text from PDFs. Install it in Colab: pip install PyPDF2')\n",
    "            text = []\n",
    "            reader = PyPDF2.PdfReader(path)\n",
    "            for page in reader.pages:\n",
    "                ptext = page.extract_text() or ''\n",
    "                text.append(ptext)\n",
    "            return '\\n\\n'.join(text)\n",
    "    try:\n",
    "        file.seek(0)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        content = file.read()\n",
    "        if isinstance(content, bytes):\n",
    "            return content.decode('utf-8', errors='ignore')\n",
    "        return str(content)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f'Unable to read uploaded file: {e}')\n",
    "\n",
    "def make_zip_from_texts(texts: List[str], names: List[str], zip_path: str) -> str:\n",
    "    with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as z:\n",
    "        for txt, name in zip(texts, names):\n",
    "            safe_name = name if name else 'summary.txt'\n",
    "            if not safe_name.lower().endswith('.txt'):\n",
    "                safe_name = safe_name + '.txt'\n",
    "            z.writestr(safe_name, txt)\n",
    "    return zip_path\n",
    "\n",
    "\n",
    "def chunk_text(text: str, max_tokens=1024, approx_chars_per_token=4) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    max_chars = max_tokens * approx_chars_per_token\n",
    "    if len(text) <= max_chars:\n",
    "        return [text]\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + max_chars, len(text))\n",
    "        if end < len(text):\n",
    "            sep = text.rfind('\\n', start, end)\n",
    "            if sep <= start:\n",
    "                sep = text.rfind('. ', start, end)\n",
    "            if sep > start:\n",
    "                end = sep + 1\n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = end\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6520975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loader and inference wrapper (uses project's config.get_model_tokenizer_device)\n",
    "import torch\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "MODEL = None\n",
    "TOKENIZER = None\n",
    "DEVICE = None\n",
    "\n",
    "def load_model(use_fp16_on_cuda=True) -> Tuple[Any, Any, torch.device]:\n",
    "    global MODEL, TOKENIZER, DEVICE\n",
    "    if MODEL is not None and TOKENIZER is not None:\n",
    "        return MODEL, TOKENIZER, DEVICE\n",
    "    try:\n",
    "        from config import get_model_tokenizer_device\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f'Failed to import get_model_tokenizer_device from config.py: {e}')\n",
    "    MODEL, TOKENIZER, DEVICE = get_model_tokenizer_device(use_fp16_on_cuda=use_fp16_on_cuda)\n",
    "    return MODEL, TOKENIZER, DEVICE\n",
    "\n",
    "def summarize_texts(texts: List[str], max_length=128, min_length=30, num_return_sequences=1, temperature=1.0, num_beams=4, device=None) -> List[List[str]]:\n",
    "    if device is None:\n",
    "        device = DEVICE or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MODEL\n",
    "    tokenizer = TOKENIZER\n",
    "    if model is None or tokenizer is None:\n",
    "        raise RuntimeError('Model/tokenizer not loaded. Call load_model() first.')\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        if not text or not text.strip():\n",
    "            results.append([''])\n",
    "            continue\n",
    "        inputs = tokenizer.encode_plus(text, return_tensors='pt', truncation=True)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs.get('attention_mask', None)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(device)\n",
    "        gen_kwargs = dict(max_length=max_length, min_length=min_length, temperature=temperature, num_return_sequences=num_return_sequences)\n",
    "        if num_beams and num_beams > 1:\n",
    "            gen_kwargs['num_beams'] = num_beams\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, **gen_kwargs)\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        grouped = [decoded[i:i+num_return_sequences] for i in range(0, len(decoded), num_return_sequences)]\n",
    "        results.append(grouped[0] if grouped else decoded)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc9c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio UI: build blocks and the summarization generator that streams progress and auto-downloads final file\n",
    "import gradio as gr\n",
    "import base64\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def _make_data_url(path: str) -> tuple[str,str]:\n",
    "    \"\"\"Return (data_url, filename) for a file path. Reads bytes and encodes to base64.\"\"\"\n",
    "    filename = Path(path).name\n",
    "    with open(path, 'rb') as fh:\n",
    "        data = fh.read()\n",
    "    b64 = base64.b64encode(data).decode('ascii')\n",
    "    data_url = f\"data:application/octet-stream;base64,{b64}\"\n",
    "    return data_url, filename\n",
    "\n",
    "def summarize_interface(uploaded_files, raw_text, selected_indices, max_length, min_length, num_return_sequences, temperature, num_beams, use_fp16):\n",
    "    \"\"\"Generator: yields (display_text, download_path|None, html_auto_download) to stream progress and trigger client download.\"\"\"\n",
    "    inputs = []\n",
    "    names = []\n",
    "    if uploaded_files:\n",
    "        for f in uploaded_files:\n",
    "            try:\n",
    "                txt = load_text_from_uploaded(f)\n",
    "            except Exception as e:\n",
    "                txt = f'<<ERROR reading file: {e}>>'\n",
    "            inputs.append(clean_text(txt))\n",
    "            names.append(getattr(f, 'name', 'uploaded'))\n",
    "    if raw_text and raw_text.strip():\n",
    "        inputs.append(clean_text(raw_text))\n",
    "        names.append('pasted_text')\n",
    "\n",
    "    if not inputs:\n",
    "        yield ('No inputs provided. Upload files or paste text.', None, '')\n",
    "        return\n",
    "\n",
    "    # Optionally filter by indices like '0,2'\n",
    "    if selected_indices and selected_indices.strip():\n",
    "        try:\n",
    "            parts = [p.strip() for p in selected_indices.split(',') if p.strip()!='']\n",
    "            idxs = [int(p) for p in parts]\n",
    "            inputs = [inputs[i] for i in idxs if 0 <= i < len(inputs)]\n",
    "            names = [names[i] for i in idxs if 0 <= i < len(names)]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        load_model(use_fp16_on_cuda=use_fp16)\n",
    "    except Exception as e:\n",
    "        yield (f'Error loading model: {e}', None, '')\n",
    "        return\n",
    "\n",
    "    summaries = []\n",
    "    tmp_dir = '/tmp'\n",
    "    for i, text in enumerate(inputs):\n",
    "        try:\n",
    "            res = summarize_texts([text], max_length=max_length, min_length=min_length, num_return_sequences=num_return_sequences, temperature=temperature, num_beams=num_beams)\n",
    "            summary_text = '\\n\\n'.join(res[0])\n",
    "            summaries.append(summary_text)\n",
    "        except Exception as e:\n",
    "            summaries.append(f'<<Error during summarization: {e}>>')\n",
    "        display = []\n",
    "        for n, s in zip(names, summaries):\n",
    "            display.append(f'--- {n} ---\\n{ s }')\n",
    "        display_text = '\\n\\n'.join(display)\n",
    "        # intermediate yields: no download yet\n",
    "        yield (display_text, None, '')\n",
    "\n",
    "    # final output files\n",
    "    if len(summaries) == 1:\n",
    "        out_path = f'{tmp_dir}/summary.txt'\n",
    "        with open(out_path, 'w', encoding='utf-8') as fh:\n",
    "            fh.write(summaries[0])\n",
    "        # create base64 data URL for auto-download\n",
    "        try:\n",
    "            data_url, filename = _make_data_url(out_path)\n",
    "            html = f'<a id=\"dl\" href=\"{data_url}\" download=\"{filename}\">Download</a><script>document.getElementById(\"dl\").click();</script>'\n",
    "        except Exception as e:\n",
    "            html = ''\n",
    "        yield ('All done', out_path, html)\n",
    "    else:\n",
    "        zip_path = f'{tmp_dir}/summaries.zip'\n",
    "        make_zip_from_texts(summaries, names, zip_path)\n",
    "        try:\n",
    "            data_url, filename = _make_data_url(zip_path)\n",
    "            html = f'<a id=\"dl\" href=\"{data_url}\" download=\"{filename}\">Download</a><script>document.getElementById(\"dl\").click();</script>'\n",
    "        except Exception as e:\n",
    "            html = ''\n",
    "        yield ('All done', zip_path, html)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown('Upload notes (.txt, .md, .pdf) or paste text. Select files to summarize and press Summarize.')\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(file_count='multiple', label='Upload note files')\n",
    "        text_input = gr.Textbox(lines=8, placeholder='Paste note text here', label='Raw text')\n",
    "    with gr.Row():\n",
    "        max_length = gr.Slider(16, 1024, value=128, step=8, label='max_length')\n",
    "        min_length = gr.Slider(8, 512, value=30, step=1, label='min_length')\n",
    "    with gr.Row():\n",
    "        num_return_sequences = gr.Slider(1, 5, value=1, step=1, label='num_return_sequences')\n",
    "        temperature = gr.Slider(0.1, 2.0, value=1.0, step=0.1, label='temperature')\n",
    "        num_beams = gr.Slider(1, 8, value=4, step=1, label='num_beams')\n",
    "    use_fp16 = gr.Checkbox(value=True, label='use_fp16_on_cuda (if GPU)')\n",
    "    selected_indices = gr.Textbox(lines=1, placeholder='e.g. 0,2 to pick first and third uploaded files (or leave empty)', label='selected_indices')\n",
    "    summarize_btn = gr.Button('Summarize')\n",
    "    output = gr.Textbox(label='Summaries (display)')\n",
    "    download_output = gr.File(label='Download results')\n",
    "    html_download = gr.HTML(label='Download (auto)')\n",
    "\n",
    "    summarize_btn.click(fn=summarize_interface, inputs=[file_input, text_input, selected_indices, max_length, min_length, num_return_sequences, temperature, num_beams, use_fp16], outputs=[output, download_output, html_download])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781fd70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio app (execute in Colab after installing requirements)\n",
    "try:\n",
    "    demo.launch(share=False)\n",
    "except Exception as e:\n",
    "    print('Failed to launch Gradio app:', e)\n",
    "    print('If running in Colab, ensure you have run the pip install cell and that the notebook has access to project files.')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
