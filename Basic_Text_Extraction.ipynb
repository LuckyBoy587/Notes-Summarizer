{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOT42ZSF3EgKqSTcATv4JIM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuckyBoy587/Notes-Summarizer/blob/master/Basic_Text_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "EHBQ6__De-80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load a T5 paraphrasing model\n",
        "model_name = \"Vamsi/T5_Paraphrase_Paws\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "def paraphrase(text, num_return_sequences=1):\n",
        "    input_text = \"paraphrase: \" + text + \" </s>\"\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        input_text,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Move inputs to GPU\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=256,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        num_beams=5,\n",
        "        temperature=1.5,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "    paraphrased = [tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "                   for output in outputs]\n",
        "\n",
        "    return paraphrased\n",
        "\n",
        "text = \"\"\"\n",
        "CHAPTER 1: MACHINE LEARNING\n",
        "Machine learning is a field of artificial intelligence. It focuses on building systems that learn from data.\n",
        "It is widely used in applications like recommendation systems and computer vision.\n",
        "\n",
        "TOPIC: DEEP LEARNING\n",
        "Deep learning uses neural networks with many layers. It has achieved state-of-the-art results in image and speech recognition.\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "yS8ImkDojeAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def split_into_topics(text):\n",
        "    lines = text.split(\"\\n\")\n",
        "    topics = {}\n",
        "    current_topic = None\n",
        "    buffer = []\n",
        "\n",
        "    def flush_buffer(topic, buf):\n",
        "        if buf:\n",
        "            # Join lines into one block\n",
        "            block = \" \".join(buf)\n",
        "            # Clean unwanted breaks/spaces\n",
        "            block = re.sub(r'\\s+', ' ', block).strip()\n",
        "            # Replace dashes/bullets with colons for readability\n",
        "            block = re.sub(r'\\s*[-–]+\\s*', ': ', block)\n",
        "            # Split into sentences\n",
        "            return sent_tokenize(block)\n",
        "        return []\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if re.match(r'^<.*>$', line):  # topic header\n",
        "            if current_topic and buffer:\n",
        "                topics[current_topic].extend(flush_buffer(current_topic, buffer))\n",
        "            topic_name = line.strip(\"<>\").strip()\n",
        "            current_topic = topic_name if topic_name else \"Unnamed Topic\"\n",
        "            topics[current_topic] = []\n",
        "            buffer = []\n",
        "        elif line:  # content line\n",
        "            buffer.append(line)\n",
        "\n",
        "    # Flush last topic\n",
        "    if current_topic and buffer:\n",
        "        topics[current_topic].extend(flush_buffer(current_topic, buffer))\n",
        "\n",
        "    return topics\n"
      ],
      "metadata": {
        "id": "tIYJM2DCkMqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = split_into_topics(text)\n",
        "for topic, sentences in topics.items():\n",
        "    print(f\"\\n{topic}:\\n\")\n",
        "    print(*paraphrase(\"\\n\".join(sentences), 1), sep=\"\\n\")"
      ],
      "metadata": {
        "id": "ntpRBWktkWy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def merge_short_sentences(text, min_words=15):\n",
        "    sentences = sent_tokenize(text)\n",
        "    print(len(sentences), sentences)\n",
        "    merged = []\n",
        "    buffer = \"\"\n",
        "\n",
        "    for sent in sentences:\n",
        "        word_count = len(sent.split())\n",
        "        if word_count < min_words:\n",
        "            buffer += \" \" + sent if buffer else sent\n",
        "        else:\n",
        "            if buffer:\n",
        "                merged.append(buffer.strip())\n",
        "                buffer = \"\"\n",
        "            merged.append(sent)\n",
        "    if buffer:\n",
        "        merged.append(buffer.strip())\n",
        "    return merged\n"
      ],
      "metadata": {
        "id": "eHuTutU_nv9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def paraphrase_chunks(chunks, model, tokenizer, device):\n",
        "    bullets = []\n",
        "    for chunk in chunks:\n",
        "        input_text = \"paraphrase: \" + chunk + \" </s>\"\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            max_length=256,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoding[\"input_ids\"].to(device)\n",
        "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=128,\n",
        "            num_beams=5,\n",
        "            num_return_sequences=1,\n",
        "            temperature=1.5,\n",
        "            top_k=50,\n",
        "            top_p=0.95\n",
        "        )\n",
        "\n",
        "        paraphrased = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "        bullets.append(paraphrased)\n",
        "    return bullets\n"
      ],
      "metadata": {
        "id": "iyMfMO3tnify"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import os\n",
        "\n",
        "def process_text_and_download(topics, filename=\"paraphrased_output.txt\"):\n",
        "    \"\"\"\n",
        "    Processes the input text by merging short sentences and paraphrasing chunks,\n",
        "    then creates a downloadable text file with the results as bullet points.\n",
        "\n",
        "    Args:\n",
        "        topics (dict): A dictionary where keys are topic names and values are lists of sentences.\n",
        "        filename (str): The name for the output downloadable file.\n",
        "    \"\"\"\n",
        "    output_content = \"\"\n",
        "    for topic, chunks in topics.items():\n",
        "        if 'model' not in globals() or 'tokenizer' not in globals() or 'device' not in globals():\n",
        "            print(\"Error: Model, tokenizer, or device not loaded. Please run the model loading cell first.\")\n",
        "            return\n",
        "        print(chunks)\n",
        "        bullets = paraphrase_chunks(chunks, model, tokenizer, device)\n",
        "\n",
        "        # Format as bullet points\n",
        "        output_content += f\"\\n## {topic}\\n\"\n",
        "        output_content += \"\\n\".join([f\"• {b}\" for b in bullets]) + \"\\n\"\n",
        "\n",
        "\n",
        "    # Create a downloadable file\n",
        "    buffer = io.BytesIO(output_content.encode())\n",
        "    # Generate a filename if not provided based on the first topic or default\n",
        "    if filename == \"paraphrased_output.txt\" and topics:\n",
        "        first_topic = list(topics.keys())[0]\n",
        "        filename = f\"{first_topic.replace(' ', '_').lower()}_paraphrased.txt\"\n",
        "    elif filename == \"paraphrased_output.txt\":\n",
        "        filename = \"paraphrased_output.txt\"\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(output_content)\n",
        "    files.download(filename=filename)\n",
        "    print(f\"Processed text and created downloadable file: {filename}\")\n",
        "\n",
        "# Example usage (commented out):\n",
        "# text = \"\"\"Genshin Impact (Chinese: 原神; pinyin: Yuán shén; lit. 'Original God') is a 2020 action role-playing game produced by miHoYo (HoYoverse).[c] The game features an anime-style open world environment and an action-based battle system using elemental magic and character-switching. A free-to-play game monetized through gacha game mechanics, Genshin Impact is updated regularly using the games as a service model; it was originally released for Android, iOS, PlayStation 4 and Windows, followed by the PlayStation 5 in 2021, with an Xbox Series X/S version in November 2024. In China, a native port for HarmonyOS NEXT was released in September 2025.\n",
        "\n",
        "# Genshin Impact takes place in the fantasy world of Teyvat, home to seven nations, each of which is tied to a different element and ruled by a different god called an \"Archon.\" The story follows the Traveler, an interstellar adventurer who, at the start of the game, is separated from their twin sibling after the two land in Teyvat. Thereafter, the Traveler journeys across the nations of Teyvat in search of the lost sibling, accompanied by their guide, Paimon. Along the way, the two befriend myriad individuals, become involved in the affairs of its nations, and begin to unravel the mysteries of the land.\n",
        "\n",
        "# Development began in 2017 and takes inspiration from a variety of sources, including The Legend of Zelda: Breath of the Wild, anime, Gnosticism, and an array of cultures and world mythologies. Genshin Impact has received generally positive reviews, with critics writing approving of its combat mechanics and its immersive open world. Conversely, some criticism has been directed at its simplistic endgame and its gacha-based monetization model. The game has also been subjected to controversy over censorship of content related to Chinese politics, allegations of colorism in character design, and privacy and security concerns. Across all platforms, the game is estimated to have grossed nearly $3.8 billion by the end of 2022, representing the highest ever first-year launch revenue for any video game.[4][5]\"\"\"\n",
        "\n",
        "# process_text_and_download(text)"
      ],
      "metadata": {
        "id": "S8F9MJ7xnpOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "import fitz  # This is the PyMuPDF library\n",
        "import re\n",
        "\n",
        "def extract_topics_from_pdf(pdf_path, write_to_file=False):\n",
        "    \"\"\"\n",
        "    Extracts content from a PDF and formats it into <TOPIC> blocks.\n",
        "\n",
        "    This function identifies topics by assuming that text with a larger-than-average\n",
        "    font size is a heading. It's a heuristic that works well for many documents!\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The file path to the PDF.\n",
        "        write_to_file (bool): If True, write the formatted content to a .txt file with the same name as the PDF.\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "    except Exception as e:\n",
        "        return f\"Error opening PDF: {e}\"\n",
        "\n",
        "    # We will store our topics and their content in a list of tuples\n",
        "    # (topic_name, content_string)\n",
        "    structured_content = []\n",
        "    current_content = []\n",
        "\n",
        "    # Let's define what we consider a \"heading\".\n",
        "    # We'll find the most common font size and assume anything\n",
        "    # a bit larger is a heading. This is our main heuristic.\n",
        "    # We can set a sensible default threshold.\n",
        "    HEADING_FONT_THRESHOLD = 14.0\n",
        "\n",
        "    for page_num, page in enumerate(doc):\n",
        "        # The 'dict' format gives us detailed info about each text block\n",
        "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "\n",
        "        for block in blocks:\n",
        "            # A block contains lines, and a line contains spans of text\n",
        "            if \"lines\" in block:\n",
        "                for line in block[\"lines\"]:\n",
        "                    for span in line[\"spans\"]:\n",
        "                        font_size = span[\"size\"]\n",
        "                        text = span[\"text\"].strip()\n",
        "\n",
        "                        # Clean up text that might be just noise\n",
        "                        if not text or len(text) < 3:\n",
        "                            continue\n",
        "\n",
        "                        # --- HEADING IDENTIFICATION LOGIC ---\n",
        "                        # If the font is larger than our threshold, we declare it a new topic!\n",
        "                        if font_size > HEADING_FONT_THRESHOLD:\n",
        "                            # First, save the content we've collected for the *previous* topic\n",
        "                            if current_content:\n",
        "                                structured_content.append((\"\".join(current_content)))\n",
        "                                current_content = [] # Reset for the new topic\n",
        "\n",
        "                            # Start a new topic\n",
        "                            # Using a special marker to distinguish topics\n",
        "                            structured_content.append(f\"<TOPIC>{text}\")\n",
        "                        else:\n",
        "                            # Otherwise, it's just regular content. Add it to the current topic's text.\n",
        "                            # We add a space to ensure words are not squished together.\n",
        "                            current_content.append(text + \" \")\n",
        "                    current_content.append(\"\\n\")  # New line after each line of text\n",
        "\n",
        "    # Don't forget to add the very last block of content after the loop ends!\n",
        "    if current_content:\n",
        "        structured_content.append(\"\".join(current_content))\n",
        "\n",
        "    doc.close()\n",
        "\n",
        "    # Filter out short or irrelevant topic sections\n",
        "    filtered_content = []\n",
        "    i = 0\n",
        "    while i < len(structured_content):\n",
        "        if structured_content[i].startswith(\"<TOPIC>\"):\n",
        "            # Check if there's a next item and if it's content\n",
        "            if i + 1 < len(structured_content) and not structured_content[i + 1].startswith(\"<TOPIC>\"):\n",
        "                content = structured_content[i + 1]\n",
        "                if len(content.strip()) >= 100:  # Minimum length threshold\n",
        "                    filtered_content.append(structured_content[i])\n",
        "                    filtered_content.append(content)\n",
        "            # Skip the topic if content is too short\n",
        "            i += 2  # Skip topic and its content\n",
        "        else:\n",
        "            # If it's standalone content (unlikely), add it\n",
        "            filtered_content.append(structured_content[i])\n",
        "            i += 1\n",
        "\n",
        "    # --- FINAL FORMATTING ---\n",
        "    # Now, let's join everything into the final string format you wanted.\n",
        "    output = \"\"\n",
        "    for item in filtered_content:\n",
        "        if item.startswith(\"<TOPIC>\"):\n",
        "            # It's a topic header\n",
        "            topic_name = item.replace(\"<TOPIC>\", \"\").strip()\n",
        "            output += f\"\\n<{topic_name.upper()}>\\n\"\n",
        "        else:\n",
        "            # It's content, clean it up a bit\n",
        "            # Replace multiple spaces/tabs with single space, preserve newlines\n",
        "            content = re.sub(r'[ \\t]+', ' ', item.strip())\n",
        "            output += content + \"\\n\"\n",
        "\n",
        "    if write_to_file:\n",
        "        file_name = \"./\" + pdf_path.split(\"\\\\\")[-1].replace('.pdf', '.txt')\n",
        "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "CwYTRnoJrCFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "def upload_and_extract_topics():\n",
        "    \"\"\"\n",
        "    Allows the user to upload a PDF and extracts topics using the existing function.\n",
        "    \"\"\"\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        try:\n",
        "            if not filename.endswith('.pdf'):\n",
        "                print(f'File \"{filename}\" is not a PDF. Skipping.')\n",
        "                continue\n",
        "            # Process the uploaded file\n",
        "            extracted_text = extract_topics_from_pdf(filename)\n",
        "            return extracted_text\n",
        "\n",
        "        finally:\n",
        "            os.remove(filename)\n",
        "# Call the function to start the upload process\n",
        "# upload_and_extract_topics()"
      ],
      "metadata": {
        "id": "1s_QHNML1GkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_pdf_content = upload_and_extract_topics()\n",
        "topics = split_into_topics(uploaded_pdf_content)\n",
        "process_text_and_download(topics)"
      ],
      "metadata": {
        "id": "3_75Kfej6Jw1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}